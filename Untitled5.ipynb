{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbFQ/ZesIkHpcdri+I0Ciy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhaskar-sinha/Business-Analytics-Datasets/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "NIFTY100 ESG: Scrape constituents, download 5-year daily prices, optimize & plot\n",
        "Ported from an R script to Python.\n",
        "\n",
        "Saves:\n",
        " - cumulative_returns_index.png\n",
        " - efficient_frontier.png\n",
        "\n",
        "Requirements:\n",
        " - pandas, numpy, yfinance, beautifulsoup4, lxml, matplotlib, pyportfolioopt, scipy, tqdm\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "from datetime import date, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from pypfopt import expected_returns, risk_models, EfficientFrontier, plotting\n",
        "from pypfopt import objective_functions\n",
        "\n",
        "# ---------------------------\n",
        "# PARAMETERS\n",
        "# ---------------------------\n",
        "YEARS_BACK = 5\n",
        "END_DATE = pd.Timestamp(date.today())\n",
        "START_DATE = END_DATE - pd.DateOffset(years=YEARS_BACK)\n",
        "MAX_MISSING_FRAC = 0.20   # drop assets with >20% missing\n",
        "RF_ANNUAL = 0.04          # example annual risk-free (use actual 10yr govt yield if available)\n",
        "YAHOO_COMPONENTS_URL = \"https://sg.finance.yahoo.com/quote/NIFTY100_ESG.NS/components/\"\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Scrape constituents list from Yahoo Finance components page\n",
        "# ---------------------------\n",
        "print(\"Scraping constituents from:\", YAHOO_COMPONENTS_URL)\n",
        "symbols_raw = []\n",
        "try:\n",
        "    resp = requests.get(YAHOO_COMPONENTS_URL, timeout=15)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "    # Find first table on the page (Yahoo components usually present a table)\n",
        "    table = soup.find(\"table\")\n",
        "    if table:\n",
        "        # parse header & rows\n",
        "        df = pd.read_html(str(table))[0]\n",
        "        # Try to detect a symbol/ticker column\n",
        "        possible_cols = [c for c in df.columns if any(x in str(c).lower() for x in [\"symbol\",\"ticker\"])]\n",
        "        if possible_cols:\n",
        "            symbols_raw = df[possible_cols[0]].astype(str).str.strip().unique().tolist()\n",
        "        else:\n",
        "            # fallback: use first column\n",
        "            symbols_raw = df.iloc[:, 0].astype(str).str.strip().unique().tolist()\n",
        "    else:\n",
        "        print(\"No table found on Yahoo page.\")\n",
        "except Exception as e:\n",
        "    print(\"Scrape failed:\", e)\n",
        "\n",
        "if not symbols_raw:\n",
        "    # Provide fallback manual list (user should edit the list if needed).\n",
        "    print(\"Using fallback manual symbols list â€” please edit if you have the constituents.\")\n",
        "    symbols_raw = [\n",
        "        # example: \"RELIANCE\", \"TCS\", \"INFY\"\n",
        "        # Put your list here if scraping failed\n",
        "    ]\n",
        "\n",
        "print(f\"Scraped {len(symbols_raw)} symbols. Sample: {symbols_raw[:10]}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Map scraped symbols to Yahoo tickers expected by yfinance\n",
        "# ---------------------------\n",
        "def to_yahoo_ticker(sym):\n",
        "    s = str(sym).strip()\n",
        "    # If already has known exchange suffix, keep as is\n",
        "    if any(s.upper().endswith(suf) for suf in [\".NS\", \".BO\", \".BOM\", \".NSE\"]):\n",
        "        return s\n",
        "    # If string includes spaces (company name), take first token\n",
        "    s2 = s.split()[0]\n",
        "    # Append .NS for NSE\n",
        "    return f\"{s2}.NS\"\n",
        "\n",
        "yahoo_tickers = [to_yahoo_ticker(s) for s in symbols_raw]\n",
        "# Remove empties and duplicates\n",
        "yahoo_tickers = [t for t in pd.unique(yahoo_tickers) if t and str(t) != \"nan\"]\n",
        "print(f\"Prepared {len(yahoo_tickers)} Yahoo-style tickers. Sample: {yahoo_tickers[:10]}\")\n",
        "\n",
        "# Manual overrides (edit if you find specific issues)\n",
        "manual_map = {\n",
        "    # \"OLD.SYM\": \"CORRECT.NS\",\n",
        "}\n",
        "if manual_map:\n",
        "    yahoo_tickers = [manual_map.get(t, t) for t in yahoo_tickers]\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Download Adjusted Close prices for each ticker (last 5 years)\n",
        "# ---------------------------\n",
        "def download_adj_close(tickers, start, end, pause=0.5):\n",
        "    frames = {}\n",
        "    for t in tqdm(tickers, desc=\"Downloading tickers\"):\n",
        "        try:\n",
        "            # polite pause\n",
        "            time.sleep(pause)\n",
        "            data = yf.download(t, start=start.strftime(\"%Y-%m-%d\"), end=(end + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
        "                               progress=False, auto_adjust=False, threads=False)\n",
        "            if data is None or data.empty:\n",
        "                # try fallback with period string\n",
        "                data = yf.download(t, period=f\"{YEARS_BACK}y\", progress=False, auto_adjust=False, threads=False)\n",
        "            if data is None or data.empty:\n",
        "                print(f\"Failed to download or empty for {t}\")\n",
        "                continue\n",
        "            if 'Adj Close' not in data.columns:\n",
        "                print(f\"No 'Adj Close' column for {t}; skipping.\")\n",
        "                continue\n",
        "            s = data['Adj Close'].rename(t)\n",
        "            frames[t] = s\n",
        "        except Exception as e:\n",
        "            print(f\"Failed: {t} -> {e}\")\n",
        "    if not frames:\n",
        "        return pd.DataFrame()\n",
        "    prices = pd.concat(frames.values(), axis=1)\n",
        "    prices.columns = list(frames.keys())\n",
        "    return prices\n",
        "\n",
        "prices = download_adj_close(yahoo_tickers, START_DATE, END_DATE, pause=0.5)\n",
        "if prices.empty:\n",
        "    raise SystemExit(\"No price series downloaded. Aborting.\")\n",
        "\n",
        "print(\"Merged price series shape:\", prices.shape)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Filter assets with too much missing data\n",
        "# ---------------------------\n",
        "missing_frac = prices.isna().mean()\n",
        "keep_assets = missing_frac[missing_frac <= MAX_MISSING_FRAC].index.tolist()\n",
        "print(f\"Keeping {len(keep_assets)} assets after missingness filter (<= {MAX_MISSING_FRAC*100}%).\")\n",
        "prices = prices[keep_assets]\n",
        "\n",
        "# Forward/backward fill small NaNs then remove remaining leading/trailing NAs\n",
        "prices = prices.ffill().bfill()\n",
        "# Remove rows that are all NA (shouldn't be after ffill/bfill)\n",
        "prices = prices.dropna(how='all')\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Compute daily log returns\n",
        "# ---------------------------\n",
        "# Align to business days (market calendar approximation)\n",
        "prices = prices.sort_index()\n",
        "# Compute log returns: log(P_t / P_{t-1})\n",
        "log_rets = np.log(prices / prices.shift(1)).dropna(how='all')\n",
        "# Drop columns that are all-NA\n",
        "valid_assets = log_rets.columns[log_rets.isna().sum() < len(log_rets)]\n",
        "log_rets = log_rets[valid_assets]\n",
        "print(f\"Final returns matrix: {log_rets.shape[1]} assets and {log_rets.shape[0]} observations.\")\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Basic exploratory plots (index-level)\n",
        "# ---------------------------\n",
        "# Average constituent daily return (mean across assets) and cumulative performance\n",
        "avg_daily = log_rets.mean(axis=1).dropna()\n",
        "cum_returns = (np.exp(avg_daily).cumprod() - 1)  # cumulative return series relative to 0\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.plot(cum_returns.index, cum_returns.values)\n",
        "plt.title(\"Average Constituent Return (Daily) - Cumulative Performance\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Cumulative return\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"cumulative_returns_index.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved cumulative_returns_index.png\")\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Portfolio optimization: Efficient frontier + Tangency (max Sharpe)\n",
        "# ---------------------------\n",
        "# We will use PyPortfolioOpt (pypfopt) to compute efficient frontier\n",
        "# Compute annualized expected returns and sample covariance from log returns\n",
        "# expected_returns.mean_historical_return expects simple returns. Convert log to simple returns:\n",
        "simple_rets = np.exp(log_rets) - 1\n",
        "\n",
        "mu = expected_returns.mean_historical_return(simple_rets, frequency=252)  # annualized mean\n",
        "S = risk_models.sample_cov(simple_rets, frequency=252)  # annualized cov matrix\n",
        "\n",
        "# Build a range of target returns across the reachable mean returns for frontier\n",
        "min_ret = mu.min()\n",
        "max_ret = mu.max()\n",
        "# create 50 target returns between min and max\n",
        "target_returns = np.linspace(min_ret, max_ret, 50)\n",
        "\n",
        "frontier_points = []\n",
        "for target in target_returns:\n",
        "    try:\n",
        "        ef = EfficientFrontier(mu, S, weight_bounds=(0, 1))  # long only, full investment enforced later\n",
        "        # find weights for target return (minimum volatility for that return)\n",
        "        weights = ef.efficient_return(target_return=target)\n",
        "        perf = ef.portfolio_performance(risk_free_rate=RF_ANNUAL, verbose=False)\n",
        "        # perf returns (expected_annual_return, annual_volatility, sharpe_ratio)\n",
        "        frontier_points.append({\n",
        "            \"target_return\": target,\n",
        "            \"mean\": perf[0],\n",
        "            \"StdDev\": perf[1],\n",
        "            \"Sharpe\": perf[2],\n",
        "            \"weights\": weights\n",
        "        })\n",
        "    except Exception:\n",
        "        # some target returns may be infeasible; skip\n",
        "        continue\n",
        "\n",
        "frontier_df = pd.DataFrame([{\n",
        "    \"mean\": p[\"mean\"],\n",
        "    \"StdDev\": p[\"StdDev\"],\n",
        "    \"Sharpe\": p[\"Sharpe\"]\n",
        "} for p in frontier_points])\n",
        "\n",
        "# Tangency portfolio: maximize Sharpe\n",
        "ef_tan = EfficientFrontier(mu, S, weight_bounds=(0, 1))\n",
        "tan_weights = ef_tan.max_sharpe(risk_free_rate=RF_ANNUAL)\n",
        "tan_perf = ef_tan.portfolio_performance(risk_free_rate=RF_ANNUAL, verbose=False)\n",
        "print(\"Tangency portfolio performance (annualized): return={:.4f}, vol={:.4f}, sharpe={:.4f}\".format(*tan_perf))\n",
        "\n",
        "# ---------------------------\n",
        "# 8) Plot efficient frontier with asset scatter\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(10, 7))\n",
        "# scatter of assets in risk-return space\n",
        "asset_returns = mu\n",
        "asset_stds = np.sqrt(np.diag(S))\n",
        "plt.scatter(asset_stds, asset_returns, marker='o', alpha=0.8)\n",
        "for i, ticker in enumerate(mu.index):\n",
        "    if i < 15:  # label only first 15 to avoid clutter; change if you want more labels\n",
        "        plt.text(asset_stds[i], asset_returns[i], ticker, fontsize=8, alpha=0.8)\n",
        "\n",
        "# plot frontier\n",
        "if not frontier_df.empty:\n",
        "    plt.plot(frontier_df[\"StdDev\"], frontier_df[\"mean\"], color='red', linewidth=2, label='Efficient frontier')\n",
        "\n",
        "# plot tangency\n",
        "plt.scatter([tan_perf[1]], [tan_perf[0]], color='green', s=80, label='Tangency (Max Sharpe)')\n",
        "\n",
        "plt.title(\"Efficient Frontier - NIFTY100 ESG Constituents\")\n",
        "plt.xlabel(\"Annualized Volatility (StdDev)\")\n",
        "plt.ylabel(\"Annualized Return (Mean)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"efficient_frontier.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved efficient_frontier.png\")\n",
        "\n",
        "# ---------------------------\n",
        "# 9) Save frontier data & tangency weights\n",
        "# ---------------------------\n",
        "# convert weights dict to series\n",
        "tan_weights_series = pd.Series(tan_weights).sort_values(ascending=False)\n",
        "frontier_summary = frontier_df.copy()\n",
        "frontier_summary.to_csv(\"frontier_summary.csv\", index=False)\n",
        "tan_weights_series.to_csv(\"tangency_weights.csv\")\n",
        "print(\"Saved frontier_summary.csv and tangency_weights.csv\")\n",
        "print(\"Top tangency weights (sample):\")\n",
        "print(tan_weights_series.head(20))\n"
      ],
      "metadata": {
        "id": "xhTFbvQ2wKG0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}